# 6 Лабораторная работа по основам глубокого обучения и искусственного интеллекта  
Используемый корпус текста: **https://lib.ru/POEEAST/ARISTOTEL/ritoriki.txt**(`settings.py`, `SOURCE =""`)  
Во время работы были небольшие трудности по поводу типа источника, при использовании отдельного файла у меня почему-то генератор текста либо отказывался генерировать продолжение фразы, либо выдавал различные артефакты без какого-либо продолжения(повторялась строчка "-й\n").  
Даже при изменении кол-ва слоев, эпох, токенов и голов внимания, результата не было, а проход по эпохам занимал вечность.    
Так что пришлось использовать ссылку на txt-файл, что немного помогло(ссылку можно поменять в `settings.py`).  
В результате работы так же присутствуют артефакты(б**о**льшая их часть связана с сайтом с которого был взят текст), что я покажу далее(см. Результаты), но в этой реализации хоть какой-то ответ присутствует.  

## Описание проекта  
Этот репозиторий содержит реализацию генератора текста на основе архитектуры **Transformer** (только декодер), обученного на различных текстах. Модель способна генерировать продолжение текста авторегрессивно, предсказывая следующее слово на основе контекста.  

## Архитектура модели  
- Реализован **Transformer Decoder с 4 слоями**  
- Размер **эмбеддингов: 256**  
- **8** голов внимания  
- Длина контекста: **128 токенов**  

## Обучение модели  
- Использован Adam оптимизатор с **learning rate 1e-4**  
- Обучение в течение **3** эпох  
- **Batch size = 1 **(из-за ограничений памяти)  

Обучение заняло примерно **10s**. с потерями по эпохам в **8.15**, **7.18**, **6.69**  
![train result](https://github.com/Gardrak/HW_basics_dp_and_ai_6/blob/main/plots/train_result.png)  

## Результаты  
После обучения модели на выборке из произведения **Аристотеля "Риторика"** был получен следующий результат генерации:  
![chat result](https://github.com/Gardrak/HW_basics_dp_and_ai_6/blob/main/plots/chat_result.png)  
Как видно на скриншоте, в выводе присутствуют артефакты, неизвестные символы - последствия изъятия док-та с сайта, где возможно была другая кодировка и/или загрязнение токинайзера, который я не смог вычистить. Скобки - последствия выбора произведения со сносками.  
Я не знаю из-за чего в тексте присутствует так много пробелов, возможно, так же из-за источника, но лучше источника на русском языке я не нашел.  

## Выводы  
### Модель успешно обучается генерировать продолжения текста на русском языке, хоть и встечаются артефакты, которые в большенстве случаев связаны с источником.  
Так, например, если поменять `SOURCE = "https://lib.ru/POEEAST/ARISTOTEL/ritoriki.txt"` на `SOURCE = "https://www.gutenberg.org/cache/epub/11/pg11.txt"` в  `settings.py` результат будет постабильней, но результат будет на английском.  

### Основные ограничения:  
- Малый размер модели (из-за ограничений оборудования)  
- Короткий контекст (**128** токенов)  
- Ограниченный объем обучающих данных

### Возможные улучшения:  
- Увеличение размера модели и длины контекста  
- Добавление **beam search ** 
- Использование предобученных весов  
- Улучшение токенизатора  
